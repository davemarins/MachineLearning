{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%matplotlib inline\n",
                "import torch\n",
                "import torchvision\n",
                "from torchvision import models\n",
                "import torchvision.transforms as transforms\n",
                "from torchvision.transforms import ToPILImage\n",
                "import torch.optim as optim\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "ROOT_PATH = \"C:\\\\Users\\\\info\\\\Documents\\\\Polito\\\\MSc\\\\ML-AI\\\\HW3\\\\data\\\\\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 3858
                },
                "colab_type": "code",
                "id": "0baK4zB_2uZz",
                "outputId": "434ed86f-4ba5-48a0-8989-340260f17bcf"
            },
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "# function to show an image\n",
                "def imshow(img):\n",
                "    img = img / 2 + 0.5     # unnormalize\n",
                "    npimg = img.numpy()\n",
                "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
                "    plt.show()\n",
                "    \n",
                "def plot_kernel(model):\n",
                "    model_weights = model.state_dict()\n",
                "    fig = plt.figure()\n",
                "    plt.figure(figsize=(10,10))\n",
                "    for idx, filt  in enumerate(model_weights['conv1.weight']):\n",
                "        if idx >= 32:\n",
                "            continue\n",
                "        plt.subplot(4,8, idx + 1)\n",
                "        plt.imshow(filt[0, :, :], cmap=\"gray\")\n",
                "        plt.axis('off')\n",
                "    plt.show()\n",
                "\n",
                "def plot_kernel_output(model,images):\n",
                "    fig1 = plt.figure()\n",
                "    plt.figure(figsize=(1,1))\n",
                "    img_normalized = (images[0] - images[0].min()) / (images[0].max() - images[0].min())\n",
                "    plt.imshow(img_normalized.numpy().transpose(1,2,0))\n",
                "    plt.show()\n",
                "    output = model.conv1(images)\n",
                "    layer_1 = output[0, :, :, :]\n",
                "    layer_1 = layer_1.data\n",
                "    fig = plt.figure()\n",
                "    plt.figure(figsize=(10,10))\n",
                "    for idx, filt  in enumerate(layer_1):\n",
                "        if idx >= 32:\n",
                "            continue\n",
                "        plt.subplot(4,8, idx + 1)\n",
                "        plt.imshow(filt, cmap=\"gray\")\n",
                "        plt.axis('off')\n",
                "    plt.show()\n",
                "\n",
                "def test_accuracy(net, dataloader):  \n",
                "    # check accuracy on whole test set\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    net.eval() # important for deactivating dropout and correctly use batchnorm accumulated statistics\n",
                "    with torch.no_grad():\n",
                "        for data in dataloader:\n",
                "            images, labels = data\n",
                "            images = images.cuda()\n",
                "            labels = labels.cuda()\n",
                "            outputs = net(images)\n",
                "            _, predicted = torch.max(outputs.data, 1)\n",
                "            total += labels.size(0)\n",
                "            correct += (predicted == labels).sum().item()\n",
                "    accuracy = 100 * correct / total\n",
                "    print(\"Accuracy of the network on the test set: {}\".format(accuracy))\n",
                "    return accuracy\n",
                "   \n",
                "n_classes = 100 \n",
                "      \n",
                "class CNN(nn.Module):\n",
                "    def __init__(self):\n",
                "        super(CNN, self).__init__()\n",
                "        #conv2d first parameter is the number of kernels at input (you get it from the output value of the previous layer)\n",
                "        #conv2d second parameter is the number of kernels you wanna have in your convolution, so it will be the n. of kernels at output.\n",
                "        #conv2d third, fourth and fifth parameters are, as you can read, kernel_size, stride and zero padding :)\n",
                "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=0)\n",
                "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0)\n",
                "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0)\n",
                "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
                "        self.conv_final = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0)\n",
                "        self.fc1 = nn.Linear(64 * 4 * 4, 4096)\n",
                "        self.fc2 = nn.Linear(4096, n_classes) #last FC for classification \n",
                "\n",
                "    def forward(self, x):\n",
                "        x = F.relu(self.conv1(x))\n",
                "        x = F.relu(self.conv2(x))\n",
                "        x = F.relu(self.conv3(x))\n",
                "        x = F.relu(self.pool(self.conv_final(x)))\n",
                "        x = x.view(x.shape[0], -1)\n",
                "        x = F.relu(self.fc1(x))\n",
                "        # hint: dropout goes here\n",
                "        x = self.fc2(x)\n",
                "        return x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# transform are heavily used to do simple and complex transformation and data augmentation\n",
                "transform_train = transforms.Compose([\n",
                "    #transforms.RandomHorizontalFlip(),\n",
                "    transforms.Resize((32,32)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
                "])\n",
                "\n",
                "transform_test = transforms.Compose([\n",
                "    transforms.Resize((32,32)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
                "])\n",
                "\n",
                "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
                "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True, num_workers=0,drop_last=True)\n",
                "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
                "testloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=False, num_workers=0,drop_last=True)\n",
                "dataiter = iter(trainloader)\n",
                "\n",
                "net = CNN()\n",
                "net = net.cuda()\n",
                "criterion = nn.CrossEntropyLoss().cuda() # it already does softmax computation for use!\n",
                "optimizer = optim.Adam(net.parameters(), lr=0.0001) # better convergency w.r.t simple SGD :)\n",
                "print('Data ready')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "n_loss_print = len(trainloader)  # print every epoch, use smaller numbers if you wanna print loss more often!\n",
                "\n",
                "n_epochs = 20\n",
                "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
                "    net.train() # important for activating dropout and correctly train batchnorm\n",
                "    running_loss = 0.0\n",
                "    for i, data in enumerate(trainloader, 0):\n",
                "        # get the inputs and cast them into cuda wrapper\n",
                "        inputs, labels = data\n",
                "        inputs = inputs.cuda()\n",
                "        labels = labels.cuda()\n",
                "        # zero the parameter gradients\n",
                "        optimizer.zero_grad()\n",
                "        # forward + backward + optimize\n",
                "        outputs = net(inputs)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        # print statistics\n",
                "        running_loss += loss.item()\n",
                "        if i % n_loss_print == (n_loss_print -1):    \n",
                "            print(\"[{}, {}] loss: {}\".format(epoch + 1, i + 1, round(running_loss / n_loss_print, 3)))\n",
                "            running_loss = 0.0\n",
                "    test_accuracy(net,testloader)\n",
                "print('Finished Training')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "n_loss_print = len(trainloader)  # print every epoch, use smaller numbers if you wanna print loss more often!\n",
                "n_epochs = 20\n",
                "stats = np.empty((n_epochs,2))\n",
                "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
                "    net.train() # important for activating dropout and correctly train batchnorm\n",
                "    running_loss = 0.0\n",
                "    for i, data in enumerate(trainloader, 0):\n",
                "        # get the inputs and cast them into cuda wrapper\n",
                "        inputs, labels = data\n",
                "        inputs = inputs.cuda()\n",
                "        labels = labels.cuda()\n",
                "        # zero the parameter gradients\n",
                "        optimizer.zero_grad()\n",
                "        # forward + backward + optimize\n",
                "        outputs = net(inputs)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        # print statistics\n",
                "        running_loss += loss.item()\n",
                "        if i % n_loss_print == (n_loss_print -1):\n",
                "            print(\"[{}, {}] loss: {}\".format(epoch + 1, i + 1, round(running_loss / n_loss_print, 3)))\n",
                "            stats[epoch][1] = running_loss / n_loss_print\n",
                "            running_loss = 0.0\n",
                "    stats[epoch][0] = test_accuracy(net,testloader)\n",
                "print('Finished Training')\n",
                "param_range = np.arange(n_epochs+1)\n",
                "plt.ylim(0.0, 100.0)\n",
                "plt.semilogx(param_range, stats[:,0], label='Accuracy', lw=lw)\n",
                "plt.xlabel('Epochs')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.legend(loc='best')\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "stats = np.empty(n_epochs)\n",
                "print(stats.shape)\n",
                "stats = np.empty((n_epochs,2))\n",
                "param_range = np.arange(n_epochs+1)\n",
                "lw = 1\n",
                "plt.ylim(0.0, 100.0)\n",
                "plt.semilogx(param_range, stats[:,0], label='Accuracy', lw=lw)\n",
                "plt.xlabel('Epochs')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.legend(loc='best')\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [],
            "name": "homework3.ipynb",
            "provenance": [],
            "version": "0.3.2"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}